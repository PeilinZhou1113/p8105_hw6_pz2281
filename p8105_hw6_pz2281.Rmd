---
title: "p8105_hw6_pz2281"
author: "Peilin Zhou"
output: github_document
---

```{r message = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(ggplot2)
```

## Problem 1

Import data for modelling

```{r}
birth_data = read.csv("./data/birthweight.csv") %>%
  as_tibble()
```

Clean the data. First check if there are missing values.

```{r}
birth_data %>% 
  summarise_all(~ sum(is.na(.)))
#no missing values


birth_data %>% 
  select(babysex, frace, mrace, malform) %>% 
  summarise_all(list(~n_distinct(.)))

#clean the data

clean_birth = birth_data %>% 
  mutate_at(c("babysex", "frace", "malform", "mrace", "fincome"), as.factor) %>% 
  mutate_if(is.integer, as.double) 
#  rename(baby_sex = babysex, baby_head = bhead, baby_length = blength, baby_weight = bweight, m_del)

```

To select independent variables for the model, I used scatter plots to observe the potential association between birth weight and other variables.

Plots for birth weight against numerical variables:
```{r}
clean_birth %>%
  select_if(is.numeric) %>% 
  gather(-bwt, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = bwt)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE) +
    facet_wrap(~ var, scales = "free") +
    theme_bw() +
    labs(
    title = "Scatter Plots of Birth Weight Against Other Numeric Variables",
    x = "Numeric Variables",
    y = "Birth Weight"
    )
```

From the plots, it can be observed that positive associations potentially exist  between birth weight and baby’s head circumference at birth, baby’s length at birth, gestational age in weeks, mother’s height gain during pregnancy. These variables can be selected as predictors of the model.

Plots for birth weight against categorical variables.

```{r}
clean_birth %>%
  select_at(c("bwt", "babysex", "frace", "malform", "mrace")) %>% 
  gather(-bwt, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = bwt)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE) +
    facet_wrap(~ var, scales = "free") +
    theme_bw() +
  labs(
    title = "Scatter Plots of Birth Weight Against Other Categorical Variables",
    x = "Categorical Variables",
    y = "Birth Weight"
  )
```

From the scatter plots, there is no clear relationship between birth weight and other categorical variables, but a visually noticeable difference in birth weight does exists based on father/mother's race, or the presence of malformations.

Based on the above observations, my hypothesized model contains 5 numeric variables and 2 categorical variables which have shown potential relationship with birth weight.

Hypothesized Model:

bwt = bhead + blength + delwt + gaweeks + wtgain + frace + mrace

```{r}
my_fit = lm(bwt ~ bhead + blength + gaweeks + wtgain + frace + mrace, data = clean_birth)
broom::glance(my_fit) %>% knitr::kable()
broom::tidy(my_fit) %>% knitr::kable()
```

However, I also want to use the step-wise model selection method(bi-directional) to fit a model and compare it with my hypothesized model. The one that has higher adjusted $r^2$ value would be selected as the hypothesized model.

Step-wise model selection:
```{r}
fit_intercept = lm(bwt ~ 1, data = clean_birth)
fit_all = lm(bwt ~ ., data = clean_birth)
fit_stepwise = step(fit_intercept, direction = 'both', scope = formula(fit_all), trace = 0)
broom::glance(fit_stepwise) %>% knitr::kable()
broom::tidy(fit_stepwise) %>% knitr::kable()
```

As shown in the table, the model fitted using step-wise selection has higher $r^2$ value. So the model I would use as hypothesized model is fit_stepwise.

Plot of model residuals against fitted values:
```{r}
clean_birth %>% 
  modelr::add_residuals(fit_stepwise) %>% 
  modelr::add_predictions(fit_stepwise) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    title = "Fitted Values vs Residuals of the Model",
    y = "Residuals",
    x = "Predicted Values"
  )
```

Most of the points are symmetrically distributed and clustered around 0 on the y-axis. However, we can see that when the predicted values are small, the residuals tend to have extreme values.

Next, compare fit_stepwise model with two prespecified models:

Fit two models:

```{r}
fit_linear = lm(bwt ~ blength + gaweeks, data = clean_birth)
broom::tidy(fit_linear) %>% knitr::kable()
broom::glance(fit_linear) %>% knitr::kable()

fit_interaction = lm(bwt ~ bhead * blength + bhead * babysex + blength * babysex + bhead * babysex * blength, data = clean_birth)
broom::tidy(fit_interaction) %>% knitr::kable()
broom::glance(fit_interaction) %>% knitr::kable()
```

Compare the models and plot the RMSE distributions:
```{r}
set.seed(1)
cv_df = crossv_mc(clean_birth,100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    fit_stepwise = map(train, ~lm(bwt ~ bhead + blength + gaweeks + wtgain + frace + mrace, data = .x)),
    fit_linear = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit_interaction = map(train, ~lm(bwt ~ bhead * blength + bhead * babysex + blength * babysex + bhead * babysex * blength, data = .x))
  ) %>% 
  mutate(
    rmse_fit_stepwise = map2_dbl(fit_stepwise, test, ~rmse(model = .x, data = .y)),
    rmse_fit_linear    = map2_dbl(fit_linear, test, ~rmse(model = .x, data = .y)),
    rmse_fit_interaction    = map2_dbl(fit_interaction, test, ~rmse(model = .x, data = .y)),
  )

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(
    title = "Root-Mean-Square-Error Distributions of Three Models"
  )
```

Based on the distributions of RMSE values from the three models, we can clearly see that the model with the fewest predictors, in other words, the fit_linear model performs the worst. And the hypothesized model, fit_stepwise, performs better than the one with interactions, but there is some overlap between the two distributions of rmse of these two models. So, it might be better to compute the average rmse to have clearer picture:

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  group_by(model) %>% 
  summarise(
    avg_rmse = mean(rmse)
  ) %>% knitr::kable()
```

Now, it is obvious that the average rmse is lowest for the hypothesized model(fit_stepwise). This model performs best comparing to the other two models.

## Problem 2

Importing the data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Bootstrapping the data:

```{r}
bs_weather = weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(fit = map(strap, ~lm(tmax ~ tmin, data = .x))) %>% 
  mutate(results = map(fit, broom::tidy)) %>% 
  mutate(results_2 = map(fit, broom::glance))
```

Making density plots and obtaining 95% confidence intervals for $\hat{r}^2$ and estimated log(beta_0 * beta_1).
```{r}
#Density plot for R^2 values
bs_weather %>% 
  unnest(results_2) %>% 
  select(r.squared) %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density() +
  labs(
    title = "Distribution of R-sqaured values",
    x = "Estimated R-sqaured"
  )
```

Under repeated sampling of the weather data, the distribution of estimated $R^2$ values of the fitted model is approximately normal and centered around 0.91.

95% confidence interval of estimated $R^2$:
```{r}
#Obtaining 95% confidence interval for R^2 estimates
bs_weather %>% 
  unnest(results_2) %>% 
  select(r.squared) %>% 
  summarise(
    ci_lower = quantile(r.squared, 0.025),
    ci_higher = quantile(r.squared, 0.975)
  ) %>% knitr::kable()
```

Distribution of estimated log(beta_0 * beta_1):
```{r}
beta_bs_weather = bs_weather %>%   
  select(-strap, -fit, -results_2) %>% 
  unnest(results) %>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate
  ) %>% 
  rename(
    id = .id, beta_0 = `(Intercept)`, beta_1 = tmin
  ) %>% 
  mutate(log_multiples = log(beta_0*beta_1))

#density plot for log(beta_0 * beta_1)
beta_bs_weather %>% 
  ggplot(aes(x = log_multiples)) +
  geom_density() +
  labs(
    title = "Distribution of log(beta_0*beta_1)",
    x = "Estimated log(beta_0*beta_1)"
  )
```

Under repeated sampling, the value of estimated log(beta_0 * beta_1) is centered around 2 to 2.025. And there is a slightly longer tail on the left, indicating having lower estimated values occasionally.

95% confidence interval for estimated log(beta_0 * beta_1):
```{r}
beta_bs_weather %>% 
  summarise(
    ci_lower = quantile(log_multiples, 0.025),
    ci_higher = quantile(log_multiples, 0.975)
  ) %>% knitr::kable()
```

